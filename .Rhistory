#data_h['Nbhd2']<-as.integer(ifelse(data_h['Nbhd']==2,1,0))
data_h['Nbhd3']<-as.integer(ifelse(data_h['Nbhd']==3,1,0))
data_h['Brick']<-as.integer(ifelse(data_h['Brick']=='Yes',1,0))
data_h<-data_h[,2:9]
lin_mod<-lm(Price~.,data=data_h)
summary(lin_mod)
data_test <- data.frame(Offers = c(2,2,2,2),
SqFt = c(1790,1790,1790,1790),
Brick = c(0,1,1,1),
Bedrooms = c(2,2,2,2),
Bathrooms = c(2,2,2,2),
Nbhd1 = c(0,0,0,0),
Nbhd3 = c(1,1,1,0))
yhat<-predict(lin_mod,data_test)
test_cases <- data_test
test_cases[,'Predicted Price'] <- yhat
print(test_cases)
data_test <- data.frame(Offers = c(2,2,2,2),
SqFt = c(1790,1790,1790,1790),
Brick = c(0,1,1,1),
Bedrooms = c(2,2,2,2),
Bathrooms = c(2,2,2,2),
Nbhd1 = c(0,0,1,0),
Nbhd3 = c(1,1,0,0))
yhat<-predict(lin_mod,data_test)
test_cases <- data_test
test_cases[,'Predicted Price'] <- yhat
print(test_cases)
knitr::opts_chunk$set(echo = TRUE)
library('ISLR2')
set.seed(1)
pairs(Boston)
attach(Boston)
lm1<-lm(crim~zn,data=Boston)
message('crim~zn: R-Square= ',round(summary(lm1)$r.squared*100,2),' Pvalue= ',round(summary(lm1)$coefficients[2,4],8))
lm2<-lm(crim~indus,data=Boston)
message('crim~indus: R-Square= ',round(summary(lm2)$r.squared*100,2),' Pvalue= ',round(summary(lm2)$coefficients[2,4],8))
lm3<-lm(crim~chas,data=Boston)
message('crim~chas: R-Square= ',round(summary(lm3)$r.squared*100,2),' Pvalue= ',round(summary(lm3)$coefficients[2,4],8))
lm4<-lm(crim~nox,data=Boston)
message('crim~nox: R-Square= ',round(summary(lm4)$r.squared*100,2),' Pvalue= ',round(summary(lm4)$coefficients[2,4],8))
lm5<-lm(crim~rm,data=Boston)
message('crim~rm: R-Square= ',round(summary(lm5)$r.squared*100,2),' Pvalue= ',round(summary(lm5)$coefficients[2,4],8))
lm6<-lm(crim~age,data=Boston)
message('crim~age: R-Square= ',round(summary(lm6)$r.squared*100,2),' Pvalue= ',round(summary(lm6)$coefficients[2,4],8))
lm7<-lm(crim~dis,data=Boston)
message('crim~dis: R-Square= ',round(summary(lm7)$r.squared*100,2),' Pvalue= ',round(summary(lm7)$coefficients[2,4],8))
lm8<-lm(crim~rad,data=Boston)
message('crim~rad: R-Square= ',round(summary(lm8)$r.squared*100,2),' Pvalue= ',round(summary(lm8)$coefficients[2,4],8))
lm9<-lm(crim~tax,data=Boston)
message('crim~tax: R-Square= ',round(summary(lm9)$r.squared*100,2),' Pvalue= ',round(summary(lm9)$coefficients[2,4],8))
lm10<-lm(crim~ptratio,data=Boston)
message('crim~ptratio: R-Square= ',round(summary(lm10)$r.squared*100,2),' Pvalue= ',round(summary(lm10)$coefficients[2,4],8))
lm11<-lm(crim~lstat,data=Boston)
message('crim~lstat: R-Square= ',round(summary(lm11)$r.squared*100,2),' Pvalue= ',round(summary(lm11)$coefficients[2,4],8))
lm12<-lm(crim~medv,data=Boston)
message('crim~medv: R-Square= ',round(summary(lm12)$r.squared*100,2),' Pvalue= ',round(summary(lm12)$coefficients[2,4],8))
plot(crim,lm8$fitted.values,xlim=c(0,20),ylim=c(0,20),ylab='Predicted Crime',xlab='Actual Crime',main='Crime Rates predicted based on Radial Highway Accessiblity')
abline(0,1,col='red')
legend(1, 20, legend="y=x",fill = "red")
plot(crim,lm9$fitted.values,xlim=c(0,20),ylim=c(0,20),ylab='Pridicted Crime',xlab='Actual Crime',main='Crime Rates predicted based on Property Tax Rates')
abline(0,1,col='red')
legend(1, 20, legend="y=x",fill = "red")
lm_multi=lm(crim~.,data=Boston)
summary(lm_multi)
names_v<-c('zn','indus','chas','nox','rm','age','dis','rad','tax','ptratio','lstat','medv')
single_coefficients<-c(summary(lm1)$coefficients[2,1],summary(lm2)$coefficients[2,1],summary(lm3)$coefficients[2,1],summary(lm4)$coefficients[2,1],summary(lm5)$coefficients[2,1],summary(lm6)$coefficients[2,1],summary(lm7)$coefficients[2,1],summary(lm8)$coefficients[2,1],summary(lm9)$coefficients[2,1],summary(lm10)$coefficients[2,1],summary(lm11)$coefficients[2,1],summary(lm12)$coefficients[2,1])
names(single_coefficients)=names_v
multi_coefficients<-summary(lm_multi)$coefficients[2:13,1]
data_coeff <- data.frame(
multi_coef = multi_coefficients,
single_coef = single_coefficients,
variable = names_v
)
# Create the bar plot
barplot(
t(data_coeff[, c("multi_coef", "single_coef")]),
beside = TRUE,
names.arg = data_coeff$variable,
las = 2,
col = c("steelblue", "orange"),  # Different colors for multi_coefficients and single_coefficients
main = "Coefficients Bar Plot",
xlab = "Variables",
ylab = "Coefficients",
legend.text = c("Multi-Variable Regression Coefficients", "Single-Variable Regression Coefficients"),
args.legend = list(x = "topright")
)
y=crim
for( i in 2:13){
#print(c('Predictor Variable is :',names_v[i-1]))
x<-Boston[,i]
x2<-x*x
x3<-x*x*x
new_dat=data.frame(y,x,x2,x3)
l_mod<-lm(y~.,data=new_dat)
#print(summary(l_mod))
}
n=nrow(College)
n1=floor(3*n/4)
n2=n-n1
ii=sample(1:n,n)
data_train=College[ii[1:n1],]
data_test=College[ii[n1+1:n2],]
message('Training Data')
summary(data_train)
message('Testing Data')
summary(data_test)
Col_mod<-lm(Apps~.,data=data_train)
Yhat<-predict(Col_mod,data_test)
test_RMSE<-sqrt(mean((Yhat-data_test$Apps)^2))
print(c("Test RMSE is =",test_RMSE))
library(glmnet)
library(ISLR2)
data("College")
n <- nrow(College)
n1 <- floor(0.75 * n)
ii <- sample(1:n, n)
data_train <- College[ii[1:n1], ]
data_test <- College[ii[(n1 + 1):n], ]
y <- data_train$Apps
x <- model.matrix(Apps ~ ., data = data_train)
new_x <- model.matrix(Apps ~ ., data = data_test)
new_y <- data_test$Apps
cv.out <- cv.glmnet(x, y, alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = bestlam)
y_hat.ridge <- predict(ridge.mod , s = bestlam , newx = new_x)
message('Best lambda = ',bestlam,' gives us an RMSE of ',sqrt(mean(( y_hat.ridge - new_y)^2)))
cv.out1 <- cv.glmnet(x, y, alpha = 1)
plot(cv.out1)
bestlam1 <- cv.out1$lambda.min
grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x, y, alpha = 0, lambda = bestlam1)
yhat.lasso <- predict(ridge.mod , s = bestlam1 , newx = new_x)
lasso <- predict(ridge.mod , s = bestlam1 , newx = new_x,type = "coefficients")[1:19,]
no_of_nz=length(lasso[lasso != 0])
lasso_RMSE=sqrt(mean(( yhat.lasso[,1] - new_y)^2))
message('Best lambda = ',bestlam,' gives us an RMSE of ',lasso_RMSE,' and number of non-zero coeff are: ',no_of_nz)
library(pls)
pcr.fit <- pcr(Apps~.,data=data_train,sclae=TRUE,validation="CV")
print(summary(pcr.fit))
validationplot(pcr.fit , val.type = "MSEP")
pcr.pred <- predict(pcr.fit , data_test, ncomp = 16)
pcr.RMSE <- sqrt(mean((pcr.pred - new_y)^2))
print(c('Best M=16','gives us an RMSE of',pcr.RMSE))
library(pls)
pls.fit <- plsr(Apps~.,data=data_train,sclae=TRUE,validation="CV")
print(summary(pls.fit))
validationplot(pls.fit , val.type = "MSEP")
pls.pred <- predict(pls.fit , data_test, ncomp = 14)
pls.RMSE <- sqrt(mean((pls.pred - new_y)^2))
print(c('Best M=14','gives us an RMSE of',pls.RMSE))
library('ISLR2')
n=nrow(Boston)
n1=floor(3*n/4)
n2=n-n1
ii=sample(1:n,n)
data_train=Boston[ii[1:n1],]
data_test=Boston[ii[n1+1:n2],]
Col_mod<-lm(crim~.,data=data_train)
Yhat<-predict(Col_mod,data_test)
test_RMSE<-sqrt(mean((Yhat-data_test$crim)^2))
print(c("Least Squares -- Test RMSE is =",test_RMSE))
library(glmnet)
library(ISLR2)
data("Boston")
n <- nrow(Boston)
n1 <- floor(0.75 * n)
ii <- sample(1:n, n)
data_train <- Boston[ii[1:n1], ]
data_test <- Boston[ii[(n1 + 1):n], ]
y <- data_train$crim
x <- model.matrix(crim ~ ., data = data_train)
new_x <- model.matrix(crim ~ ., data = data_test)
new_y <- data_test$crim
cv.out <- cv.glmnet(x, y, alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = bestlam)
y_hat.ridge <- predict(ridge.mod , s = bestlam , newx = new_x)
print(c(' Ridge Regression -- Best lambda=',bestlam,'give us an RMSE of',sqrt(mean(( y_hat.ridge - new_y)^2))))
cv.out1 <- cv.glmnet(x, y, alpha = 1)
plot(cv.out1)
bestlam1 <- cv.out1$lambda.min
grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x, y, alpha = 0, lambda = bestlam1)
yhat.lasso <- predict(ridge.mod , s = bestlam1 , newx = new_x)
lasso <- predict(ridge.mod , s = bestlam1 , newx = new_x,type = "coefficients")[1:13,]
no_of_nz=length(lasso[lasso != 0])
lasso_RMSE=sqrt(mean(( yhat.lasso[,1] - new_y)^2))
print(c('Lasso Regression -- Best lambda=',bestlam,'give us an RMSE of',lasso_RMSE,'Number of non-zero coeff are:',no_of_nz))
library(pls)
pcr.fit <- pcr(crim~.,data=data_train,sclae=TRUE,validation="CV")
print(summary(pcr.fit))
validationplot(pcr.fit , val.type = "MSEP")
pcr.pred <- predict(pcr.fit , data_test, ncomp = 9)
pcr.RMSE <- sqrt(mean((pcr.pred - new_y)^2))
print(c('PCR -- Best M=9','gives us an RMSE of',pcr.RMSE))
library(pls)
pls.fit <- plsr(crim~.,data=data_train,sclae=TRUE,validation="CV")
print(summary(pls.fit))
validationplot(pls.fit , val.type = "MSEP")
pls.pred <- predict(pls.fit , data_test, ncomp = 9)
pls.RMSE <- sqrt(mean((pls.pred - new_y)^2))
print(c('PLS -- Best M=9','gives us an RMSE of',pls.RMSE))
library(tree)
library('ISLR2')
set.seed(1)
tops<-Carseats
attach(tops)
#Sampling
n=nrow(tops)
n1=floor(3*n/4)
n2=n-n1
ii=sample(1:n,n)
toptrain=tops[ii[1:n1],]
toptest=tops[ii[n1+1:n2],]
tree.tops<-tree(Sales~.,toptrain,mindev=0.0001)
summary(tree.tops)
plot(tree.tops)
text(tree.tops,pretty=0)
yhat<-predict(tree.tops,newdata = toptest)
Sales.test<-toptest[,"Sales"]
#plot(yhat,Sales.test)
#abline(0,1)
testMSE<-sqrt(mean((yhat-Sales.test)^2))
print(c('Linear regression Test MSE is =',testMSE))
cv.tops<-cv.tree(tree.tops,K=5)
plot(cv.tops$size,cv.tops$dev,type="b")
besttree=cv.tops$size[which.min(cv.tops$dev)]
p.tops<-prune.tree(tree.tops,best=besttree)
yhat.p<-predict(p.tops,newdata = toptest)
testMSE.p<-sqrt(mean((yhat.p-Sales.test)^2))
print(c('Optimal tree-size of:',besttree,'gives us Test RMSE of',testMSE.p))
library(randomForest)
bag.tops<-randomForest(Sales~.,data = toptrain,mtry=3,importance=TRUE)
yhat.bag<-predict(bag.tops,newdata=toptest)
#plot(yhat.bag,Sales.test)
#abline(0,1)
testMSE.bag=sqrt(mean((yhat.bag-Sales.test)^2))
importance(bag.tops)
varImpPlot(bag.tops)
print(c('Bagging gives a test RMSE of :',testMSE.bag))
print(c('Most important variables are Price and ShelveLoc'))
rf.tops<-randomForest(Sales~.,data = toptrain,mtry=3,importance=TRUE)
yhat.rf<-predict(rf.tops,newdata=toptest)
#plot(yhat.rf,Sales.test)
#abline(0,1)
testMSE.rf=sqrt(mean((yhat.rf-Sales.test)^2))
importance(rf.tops)
varImpPlot(rf.tops)
print(c('Random Forest gives a test RMSE of :',testMSE.rf))
print(c('Most important variables are Price and ShelveLoc'))
library(BART)
xtrain<-toptrain[,2:11]
ytrain<-toptrain[,1]
xtest<-toptest[,2:11]
ytest<-toptest[,1]
bartfit<-gbart(xtrain,ytrain,x.test=xtest)
yhat.bart<-bartfit$yhat.test.mean
testMSE.bart<-sqrt(mean((ytest-yhat.bart)^2))
print(c('BART gives a test RMSE of :',testMSE.bart))
library(tree)
library('ISLR2')
set.seed(1)
tops<-Caravan
attach(tops)
n=nrow(tops)
n1=1000
n2=n-n1
ii=sample(1:n,n)
toptrain=tops[ii[1:n1],]
toptest=tops[ii[n1+1:n2],]
library(caret)
library(gbm)
toptrain$Purchase<-ifelse(toptrain$Purchase=='Yes',1,0)
boosted<-gbm(Purchase~.,data=toptrain,distribution="bernoulli",n.trees=1000,shrinkage=0.1)
summary(boosted)
yhat.prob=predict(boosted,newdata=toptest,n.trees=1000)
yhat.boost=ifelse(yhat.prob>0.2,'Yes','No')
confusionMatrix(as.factor(yhat.boost),as.factor(toptest$Purchase))
library(class)
mod.knn=knn(toptrain[,1:85],toptest[,1:85],toptrain[,86],k=10)
mod.knn=as.factor(ifelse(mod.knn=='1','Yes','No'))
confusionMatrix(as.factor(mod.knn),as.factor(toptest$Purchase))
library('ISLR2')
n=nrow(Default)
n1=floor(3*n/4)
n2=n-n1
ii=sample(1:n,n)
toptrain=Default[ii[1:n1],]
toptest=Default[ii[n1+1:n2],]
#summary(toptrain)
###standardize the x's
minv = rep(0,3)
maxv = rep(0,3)
toptrain_sc = toptrain
for(i in 3:4) {
minv[i] = min(toptrain[[i]])
maxv[i] = max(toptrain[[i]])
toptrain_sc[[i]] = (toptrain[[i]]-minv[i])/(maxv[i]-minv[i])
}
toptest_sc = toptest
for(i in 3:4) {
minv[i] = min(toptest[[i]])
maxv[i] = max(toptest[[i]])
toptest_sc[[i]] = (toptest[[i]]-minv[i])/(maxv[i]-minv[i])
}
### nn library
library(nnet)
set.seed(99)
znn = nnet(default~.,toptrain_sc,size=10,decay=1)
fznn = predict(znn,toptest_sc)
#summary(fznn)
lin_mod=glm(default~.,data=toptrain_sc,family=binomial)
lm_prob=predict(lin_mod,toptest_sc[2:4],type='response')
nn_pred=as.factor(ifelse(fznn>0.5,'Yes','No'))
lm_pred=as.factor(ifelse(lm_prob>0.5,'Yes','No'))
confusionMatrix(toptest_sc$default,nn_pred)
confusionMatrix(toptest_sc$default,lm_pred)
data_b<-read.csv("BeautyData.csv",header = TRUE)
plot(data_b$CourseEvals,data_b$BeautyScore,xlab='Course ratings',ylab='Beauty Score')
ll=lm(CourseEvals~BeautyScore,data=data_b)
summary(ll)
y=predict(ll,data_b[2])
par(mfrow=c(1,2))
plot(data_b$CourseEvals,y,xlab='Actual Course Ratings',ylab='Predicted Course Ratings',main='Actual vs Predicted Course Ratings')
abline(0,1,col='red')
plot(data_b$BeautyScore,data_b$CourseEvals-y,main='Residual Plot',xlab='Beauty Score',ylab='Residuals')
ll=lm(CourseEvals~.,data=data_b)
summary(ll)
y=predict(ll,data_b[2:6])
plot(data_b$CourseEvals,y,xlab='Actual Course Ratings',ylab='Predicted Course Ratings')
abline(0,1,col='red')
library(randomForest)
rf.beauty<-randomForest(CourseEvals~.,data = data_b,mtry=2,importance=TRUE)
yhat.rf<-predict(rf.beauty,newdata=data_b)
#plot(yhat.rf,Sales.test)
#abline(0,1)
RMSE.rf=sqrt(mean((yhat.rf-data_b$CourseEvals)^2))
importance(rf.beauty)
varImpPlot(rf.beauty)
data_h<-read.csv("MidCity.csv",header = TRUE)
data_h<-data_h[,2:8]
data_h['Nbhd1']<-as.integer(ifelse(data_h['Nbhd']==1,1,0))
#data_h['Nbhd2']<-as.integer(ifelse(data_h['Nbhd']==2,1,0))
data_h['Nbhd3']<-as.integer(ifelse(data_h['Nbhd']==3,1,0))
data_h['Brick']<-as.integer(ifelse(data_h['Brick']=='Yes',1,0))
data_h<-data_h[,2:9]
lin_mod<-lm(Price~.,data=data_h)
summary(lin_mod)
data_test <- data.frame(Offers = c(2,2,2,2),
SqFt = c(1790,1790,1790,1790),
Brick = c(0,1,1,1),
Bedrooms = c(2,2,2,2),
Bathrooms = c(2,2,2,2),
Nbhd1 = c(0,0,1,0),
Nbhd3 = c(1,1,0,0))
yhat<-predict(lin_mod,data_test)
test_cases <- data_test
test_cases[,'Predicted Price'] <- yhat
print(test_cases)
winequality.white <- read.csv("~/Desktop/winequality-white.csv", sep=";")
View(winequality.white)
wine_w<-wine_w %>% distinct()
library(tidyverse)
wine_w<-wine_w %>% distinct()
wine_w<-winequality.white %>% distinct()
#wine_r$alcohol2<-0
#wine_r$alcohol3<-0
wine_w$quality<-ifelse(wine_w$quality>6,1,0)
summary(wine_w)
glm.fit = glm(quality~.,data=wine_w,family=binomial)
summary(glm.fit)
wine_r$alcohol2<-wine_w$alcohol^2
wine_w$alcohol2<-wine_w$alcohol^2
wine_w$alcohol3<-wine_w$alcohol^3
glm.fit = glm(quality~.,data=wine_w,family=binomial)
summary(glm.fit)
wine_r <- read.csv("~/Desktop/winequality-red.csv", sep=";")
View(wine_r)
wine_r<-wine_r %>% distinct()
wine_r$quality<-ifelse(wine_r$quality>6,1,0)
#wine_r$alcohol2<-0
#wine_r$alcohol3<-0
glm.fit = glm(quality~.,data=wine_r,family=binomial)
summary(glm.fit)
winequality.white <- read.csv("~/Desktop/winequality-white.csv", sep=";")
View(winequality.white)
winequality.red <- read.csv("~/Desktop/winequality-red.csv", sep=";")
View(winequality.red)
summary(winequality.white)
summary(winequality.red)
library(tidyverse)
library(igraph)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
# Association rule mining
# Adapted from code by Matt Taddy
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
g_raw = read.csv("../data/groceries.txt",header=FALSE)
setwd("~/Documents/GitHub/Intro-to-ML-Exam")
library(tidyverse)
library(igraph)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
# Association rule mining
# Adapted from code by Matt Taddy
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
g_raw = read.csv("../data/groceries.txt",header=FALSE)
library(tidyverse)
library(igraph)
library(arules)  # has a big ecosystem of packages built around it
library(arulesViz)
# Association rule mining
# Adapted from code by Matt Taddy
# Read in playlists from users
# This is in "long" format -- every row is a single artist-listener pair
g_raw = read.csv("/Users/mandeep/Documents/GitHub/STA380/data/groceries.txt",header=FALSE)
long_df <- g_raw %>%
mutate(cart_id = row_number()) %>%
pivot_longer(cols = starts_with("V"), names_to = "item_position", values_to = "item_name") %>%
arrange(cart_id, as.numeric(gsub("V", "", item_position))) %>%
select(-item_position)
x=long_df$item_name != ""
long_df<-subset(long_df,x)
## Cast this variable as a special arules "transactions" class.
playtrans = as(split(long_df$item_name, long_df$cart_id), "transactions")
summary(playtrans)
# Now run the 'apriori' algorithm
# Look at rules with support > .005 & confidence >.1 & length (# artists) <= 4
musicrules = apriori(playtrans,
parameter=list(support=.005, confidence=.1, maxlen=4))
# Look at the output... so many rules!
inspect(musicrules)
## Choose a subset
inspect(subset(musicrules, subset=lift > 5))
inspect(subset(musicrules, subset=confidence > 0.6))
inspect(subset(musicrules, subset=lift > 10 & confidence > 0.5))
# plot all the rules in (support, confidence) space
# notice that high lift rules tend to have low support
plot(musicrules)
# can swap the axes and color scales
plot(musicrules, measure = c("support", "lift"), shading = "confidence")
# "two key" plot: coloring is by size (order) of item set
plot(musicrules, method='two-key plot')
# can now look at subsets driven by the plot
inspect(subset(musicrules, support > 0.035))
inspect(subset(musicrules, confidence > 0.6))
inspect(subset(musicrules, lift > 20))
# graph-based visualization
# export
# associations are represented as edges
# For rules, each item in the LHS is connected
# with a directed edge to the item in the RHS.
playlists_graph = associations2igraph(subset(musicrules, lift>1), associationsAsNodes = FALSE)
igraph::write_graph(playlists_graph, file='groceries.graphml', format = "graphml")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(igraph)
library(arules)
library(arulesViz)
g_raw = read.csv("/Users/mandeep/Documents/GitHub/STA380/data/groceries.txt",header=FALSE)
long_df <- g_raw %>%
mutate(cart_id = row_number()) %>%
pivot_longer(cols = starts_with("V"), names_to = "item_position", values_to = "item_name") %>%
arrange(cart_id, as.numeric(gsub("V", "", item_position))) %>%
select(-item_position)
x=long_df$item_name != ""
long_df<-subset(long_df,x)
playtrans = as(split(long_df$item_name, long_df$cart_id), "transactions")
summary(playtrans)
musicrules = apriori(playtrans,
parameter=list(support=.005, confidence=.1, maxlen=4))
inspect(musicrules)
playlists_graph = associations2igraph(subset(musicrules, lift>1), associationsAsNodes = FALSE)
igraph::write_graph(playlists_graph, file='groceries.graphml', format = "graphml")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(igraph)
library(arules)
library(arulesViz)
g_raw = read.csv("/Users/mandeep/Documents/GitHub/STA380/data/groceries.txt",header=FALSE)
long_df <- g_raw %>%
mutate(cart_id = row_number()) %>%
pivot_longer(cols = starts_with("V"), names_to = "item_position", values_to = "item_name") %>%
arrange(cart_id, as.numeric(gsub("V", "", item_position))) %>%
select(-item_position)
x=long_df$item_name != ""
long_df<-subset(long_df,x)
playtrans = as(split(long_df$item_name, long_df$cart_id), "transactions")
summary(playtrans)
musicrules = apriori(playtrans,
parameter=list(support=.005, confidence=.1, maxlen=4))
inspect(subset(musicrules, lift > 5))
playlists_graph = associations2igraph(subset(musicrules, lift>1), associationsAsNodes = FALSE)
igraph::write_graph(playlists_graph, file='groceries.graphml', format = "graphml")
### Let's try to plot our network now
![!Grocery Network]('./Grocerygraph.png')
### Let's try to plot our network now
![Grocery Network]('./Grocerygraph.png')
### Let's try to plot our network now
![Grocery Network]('./Grocerygraph.png')
### Let's try to plot our network now
('./Grocerygraph.png')
### Let's try to plot our network now
![alt text here](/Users/mandeep/Documents/GitHub/Intro-to-ML-Exam/Grocery_graph.png)
