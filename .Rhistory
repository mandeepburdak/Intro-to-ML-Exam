test_cases[,'Predicted Price'] <- yhat
print(test_cases)
data_test <- data.frame(Offers = c(2,2,2,2),
SqFt = c(1790,1790,1790,1790),
Brick = c(0,1,1,1),
Bedrooms = c(2,2,2,2),
Bathrooms = c(2,2,2,2),
Nbhd1 = c(0,0,1,0),
Nbhd3 = c(1,1,0,0))
yhat<-predict(lin_mod,data_test)
test_cases <- data_test
test_cases[,'Predicted Price'] <- yhat
print(test_cases)
knitr::opts_chunk$set(echo = TRUE)
library('ISLR2')
set.seed(1)
pairs(Boston)
attach(Boston)
lm1<-lm(crim~zn,data=Boston)
message('crim~zn: R-Square= ',round(summary(lm1)$r.squared*100,2),' Pvalue= ',round(summary(lm1)$coefficients[2,4],8))
lm2<-lm(crim~indus,data=Boston)
message('crim~indus: R-Square= ',round(summary(lm2)$r.squared*100,2),' Pvalue= ',round(summary(lm2)$coefficients[2,4],8))
lm3<-lm(crim~chas,data=Boston)
message('crim~chas: R-Square= ',round(summary(lm3)$r.squared*100,2),' Pvalue= ',round(summary(lm3)$coefficients[2,4],8))
lm4<-lm(crim~nox,data=Boston)
message('crim~nox: R-Square= ',round(summary(lm4)$r.squared*100,2),' Pvalue= ',round(summary(lm4)$coefficients[2,4],8))
lm5<-lm(crim~rm,data=Boston)
message('crim~rm: R-Square= ',round(summary(lm5)$r.squared*100,2),' Pvalue= ',round(summary(lm5)$coefficients[2,4],8))
lm6<-lm(crim~age,data=Boston)
message('crim~age: R-Square= ',round(summary(lm6)$r.squared*100,2),' Pvalue= ',round(summary(lm6)$coefficients[2,4],8))
lm7<-lm(crim~dis,data=Boston)
message('crim~dis: R-Square= ',round(summary(lm7)$r.squared*100,2),' Pvalue= ',round(summary(lm7)$coefficients[2,4],8))
lm8<-lm(crim~rad,data=Boston)
message('crim~rad: R-Square= ',round(summary(lm8)$r.squared*100,2),' Pvalue= ',round(summary(lm8)$coefficients[2,4],8))
lm9<-lm(crim~tax,data=Boston)
message('crim~tax: R-Square= ',round(summary(lm9)$r.squared*100,2),' Pvalue= ',round(summary(lm9)$coefficients[2,4],8))
lm10<-lm(crim~ptratio,data=Boston)
message('crim~ptratio: R-Square= ',round(summary(lm10)$r.squared*100,2),' Pvalue= ',round(summary(lm10)$coefficients[2,4],8))
lm11<-lm(crim~lstat,data=Boston)
message('crim~lstat: R-Square= ',round(summary(lm11)$r.squared*100,2),' Pvalue= ',round(summary(lm11)$coefficients[2,4],8))
lm12<-lm(crim~medv,data=Boston)
message('crim~medv: R-Square= ',round(summary(lm12)$r.squared*100,2),' Pvalue= ',round(summary(lm12)$coefficients[2,4],8))
plot(crim,lm8$fitted.values,xlim=c(0,20),ylim=c(0,20),ylab='Predicted Crime',xlab='Actual Crime',main='Crime Rates predicted based on Radial Highway Accessiblity')
abline(0,1,col='red')
legend(1, 20, legend="y=x",fill = "red")
plot(crim,lm9$fitted.values,xlim=c(0,20),ylim=c(0,20),ylab='Pridicted Crime',xlab='Actual Crime',main='Crime Rates predicted based on Property Tax Rates')
abline(0,1,col='red')
legend(1, 20, legend="y=x",fill = "red")
lm_multi=lm(crim~.,data=Boston)
summary(lm_multi)
names_v<-c('zn','indus','chas','nox','rm','age','dis','rad','tax','ptratio','lstat','medv')
single_coefficients<-c(summary(lm1)$coefficients[2,1],summary(lm2)$coefficients[2,1],summary(lm3)$coefficients[2,1],summary(lm4)$coefficients[2,1],summary(lm5)$coefficients[2,1],summary(lm6)$coefficients[2,1],summary(lm7)$coefficients[2,1],summary(lm8)$coefficients[2,1],summary(lm9)$coefficients[2,1],summary(lm10)$coefficients[2,1],summary(lm11)$coefficients[2,1],summary(lm12)$coefficients[2,1])
names(single_coefficients)=names_v
multi_coefficients<-summary(lm_multi)$coefficients[2:13,1]
data_coeff <- data.frame(
multi_coef = multi_coefficients,
single_coef = single_coefficients,
variable = names_v
)
# Create the bar plot
barplot(
t(data_coeff[, c("multi_coef", "single_coef")]),
beside = TRUE,
names.arg = data_coeff$variable,
las = 2,
col = c("steelblue", "orange"),  # Different colors for multi_coefficients and single_coefficients
main = "Coefficients Bar Plot",
xlab = "Variables",
ylab = "Coefficients",
legend.text = c("Multi-Variable Regression Coefficients", "Single-Variable Regression Coefficients"),
args.legend = list(x = "topright")
)
y=crim
for( i in 2:13){
#print(c('Predictor Variable is :',names_v[i-1]))
x<-Boston[,i]
x2<-x*x
x3<-x*x*x
new_dat=data.frame(y,x,x2,x3)
l_mod<-lm(y~.,data=new_dat)
#print(summary(l_mod))
}
n=nrow(College)
n1=floor(3*n/4)
n2=n-n1
ii=sample(1:n,n)
data_train=College[ii[1:n1],]
data_test=College[ii[n1+1:n2],]
message('Training Data')
summary(data_train)
message('Testing Data')
summary(data_test)
Col_mod<-lm(Apps~.,data=data_train)
Yhat<-predict(Col_mod,data_test)
test_RMSE<-sqrt(mean((Yhat-data_test$Apps)^2))
print(c("Test RMSE is =",test_RMSE))
library(glmnet)
library(ISLR2)
data("College")
n <- nrow(College)
n1 <- floor(0.75 * n)
ii <- sample(1:n, n)
data_train <- College[ii[1:n1], ]
data_test <- College[ii[(n1 + 1):n], ]
y <- data_train$Apps
x <- model.matrix(Apps ~ ., data = data_train)
new_x <- model.matrix(Apps ~ ., data = data_test)
new_y <- data_test$Apps
cv.out <- cv.glmnet(x, y, alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = bestlam)
y_hat.ridge <- predict(ridge.mod , s = bestlam , newx = new_x)
message('Best lambda = ',bestlam,' gives us an RMSE of ',sqrt(mean(( y_hat.ridge - new_y)^2)))
cv.out1 <- cv.glmnet(x, y, alpha = 1)
plot(cv.out1)
bestlam1 <- cv.out1$lambda.min
grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x, y, alpha = 0, lambda = bestlam1)
yhat.lasso <- predict(ridge.mod , s = bestlam1 , newx = new_x)
lasso <- predict(ridge.mod , s = bestlam1 , newx = new_x,type = "coefficients")[1:19,]
no_of_nz=length(lasso[lasso != 0])
lasso_RMSE=sqrt(mean(( yhat.lasso[,1] - new_y)^2))
message('Best lambda = ',bestlam,' gives us an RMSE of ',lasso_RMSE,' and number of non-zero coeff are: ',no_of_nz)
library(pls)
pcr.fit <- pcr(Apps~.,data=data_train,sclae=TRUE,validation="CV")
print(summary(pcr.fit))
validationplot(pcr.fit , val.type = "MSEP")
pcr.pred <- predict(pcr.fit , data_test, ncomp = 16)
pcr.RMSE <- sqrt(mean((pcr.pred - new_y)^2))
print(c('Best M=16','gives us an RMSE of',pcr.RMSE))
library(pls)
pls.fit <- plsr(Apps~.,data=data_train,sclae=TRUE,validation="CV")
print(summary(pls.fit))
validationplot(pls.fit , val.type = "MSEP")
pls.pred <- predict(pls.fit , data_test, ncomp = 14)
pls.RMSE <- sqrt(mean((pls.pred - new_y)^2))
print(c('Best M=14','gives us an RMSE of',pls.RMSE))
library('ISLR2')
n=nrow(Boston)
n1=floor(3*n/4)
n2=n-n1
ii=sample(1:n,n)
data_train=Boston[ii[1:n1],]
data_test=Boston[ii[n1+1:n2],]
Col_mod<-lm(crim~.,data=data_train)
Yhat<-predict(Col_mod,data_test)
test_RMSE<-sqrt(mean((Yhat-data_test$crim)^2))
print(c("Least Squares -- Test RMSE is =",test_RMSE))
library(glmnet)
library(ISLR2)
data("Boston")
n <- nrow(Boston)
n1 <- floor(0.75 * n)
ii <- sample(1:n, n)
data_train <- Boston[ii[1:n1], ]
data_test <- Boston[ii[(n1 + 1):n], ]
y <- data_train$crim
x <- model.matrix(crim ~ ., data = data_train)
new_x <- model.matrix(crim ~ ., data = data_test)
new_y <- data_test$crim
cv.out <- cv.glmnet(x, y, alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = bestlam)
y_hat.ridge <- predict(ridge.mod , s = bestlam , newx = new_x)
print(c(' Ridge Regression -- Best lambda=',bestlam,'give us an RMSE of',sqrt(mean(( y_hat.ridge - new_y)^2))))
cv.out1 <- cv.glmnet(x, y, alpha = 1)
plot(cv.out1)
bestlam1 <- cv.out1$lambda.min
grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x, y, alpha = 0, lambda = bestlam1)
yhat.lasso <- predict(ridge.mod , s = bestlam1 , newx = new_x)
lasso <- predict(ridge.mod , s = bestlam1 , newx = new_x,type = "coefficients")[1:13,]
no_of_nz=length(lasso[lasso != 0])
lasso_RMSE=sqrt(mean(( yhat.lasso[,1] - new_y)^2))
print(c('Lasso Regression -- Best lambda=',bestlam,'give us an RMSE of',lasso_RMSE,'Number of non-zero coeff are:',no_of_nz))
library(pls)
pcr.fit <- pcr(crim~.,data=data_train,sclae=TRUE,validation="CV")
print(summary(pcr.fit))
validationplot(pcr.fit , val.type = "MSEP")
pcr.pred <- predict(pcr.fit , data_test, ncomp = 9)
pcr.RMSE <- sqrt(mean((pcr.pred - new_y)^2))
print(c('PCR -- Best M=9','gives us an RMSE of',pcr.RMSE))
library(pls)
pls.fit <- plsr(crim~.,data=data_train,sclae=TRUE,validation="CV")
print(summary(pls.fit))
validationplot(pls.fit , val.type = "MSEP")
pls.pred <- predict(pls.fit , data_test, ncomp = 9)
pls.RMSE <- sqrt(mean((pls.pred - new_y)^2))
print(c('PLS -- Best M=9','gives us an RMSE of',pls.RMSE))
library(tree)
library('ISLR2')
set.seed(1)
tops<-Carseats
attach(tops)
#Sampling
n=nrow(tops)
n1=floor(3*n/4)
n2=n-n1
ii=sample(1:n,n)
toptrain=tops[ii[1:n1],]
toptest=tops[ii[n1+1:n2],]
tree.tops<-tree(Sales~.,toptrain,mindev=0.0001)
summary(tree.tops)
plot(tree.tops)
text(tree.tops,pretty=0)
yhat<-predict(tree.tops,newdata = toptest)
Sales.test<-toptest[,"Sales"]
#plot(yhat,Sales.test)
#abline(0,1)
testMSE<-sqrt(mean((yhat-Sales.test)^2))
print(c('Linear regression Test MSE is =',testMSE))
cv.tops<-cv.tree(tree.tops,K=5)
plot(cv.tops$size,cv.tops$dev,type="b")
besttree=cv.tops$size[which.min(cv.tops$dev)]
p.tops<-prune.tree(tree.tops,best=besttree)
yhat.p<-predict(p.tops,newdata = toptest)
testMSE.p<-sqrt(mean((yhat.p-Sales.test)^2))
print(c('Optimal tree-size of:',besttree,'gives us Test RMSE of',testMSE.p))
library(randomForest)
bag.tops<-randomForest(Sales~.,data = toptrain,mtry=3,importance=TRUE)
yhat.bag<-predict(bag.tops,newdata=toptest)
#plot(yhat.bag,Sales.test)
#abline(0,1)
testMSE.bag=sqrt(mean((yhat.bag-Sales.test)^2))
importance(bag.tops)
varImpPlot(bag.tops)
print(c('Bagging gives a test RMSE of :',testMSE.bag))
print(c('Most important variables are Price and ShelveLoc'))
rf.tops<-randomForest(Sales~.,data = toptrain,mtry=3,importance=TRUE)
yhat.rf<-predict(rf.tops,newdata=toptest)
#plot(yhat.rf,Sales.test)
#abline(0,1)
testMSE.rf=sqrt(mean((yhat.rf-Sales.test)^2))
importance(rf.tops)
varImpPlot(rf.tops)
print(c('Random Forest gives a test RMSE of :',testMSE.rf))
print(c('Most important variables are Price and ShelveLoc'))
library(BART)
xtrain<-toptrain[,2:11]
ytrain<-toptrain[,1]
xtest<-toptest[,2:11]
ytest<-toptest[,1]
bartfit<-gbart(xtrain,ytrain,x.test=xtest)
yhat.bart<-bartfit$yhat.test.mean
testMSE.bart<-sqrt(mean((ytest-yhat.bart)^2))
print(c('BART gives a test RMSE of :',testMSE.bart))
library(tree)
library('ISLR2')
set.seed(1)
tops<-Caravan
attach(tops)
n=nrow(tops)
n1=1000
n2=n-n1
ii=sample(1:n,n)
toptrain=tops[ii[1:n1],]
toptest=tops[ii[n1+1:n2],]
library(caret)
library(gbm)
toptrain$Purchase<-ifelse(toptrain$Purchase=='Yes',1,0)
boosted<-gbm(Purchase~.,data=toptrain,distribution="bernoulli",n.trees=1000,shrinkage=0.1)
summary(boosted)
yhat.prob=predict(boosted,newdata=toptest,n.trees=1000)
yhat.boost=ifelse(yhat.prob>0.2,'Yes','No')
confusionMatrix(as.factor(yhat.boost),as.factor(toptest$Purchase))
library(class)
mod.knn=knn(toptrain[,1:85],toptest[,1:85],toptrain[,86],k=10)
mod.knn=as.factor(ifelse(mod.knn=='1','Yes','No'))
confusionMatrix(as.factor(mod.knn),as.factor(toptest$Purchase))
library('ISLR2')
n=nrow(Default)
n1=floor(3*n/4)
n2=n-n1
ii=sample(1:n,n)
toptrain=Default[ii[1:n1],]
toptest=Default[ii[n1+1:n2],]
#summary(toptrain)
###standardize the x's
minv = rep(0,3)
maxv = rep(0,3)
toptrain_sc = toptrain
for(i in 3:4) {
minv[i] = min(toptrain[[i]])
maxv[i] = max(toptrain[[i]])
toptrain_sc[[i]] = (toptrain[[i]]-minv[i])/(maxv[i]-minv[i])
}
toptest_sc = toptest
for(i in 3:4) {
minv[i] = min(toptest[[i]])
maxv[i] = max(toptest[[i]])
toptest_sc[[i]] = (toptest[[i]]-minv[i])/(maxv[i]-minv[i])
}
### nn library
library(nnet)
set.seed(99)
znn = nnet(default~.,toptrain_sc,size=10,decay=1)
fznn = predict(znn,toptest_sc)
#summary(fznn)
lin_mod=glm(default~.,data=toptrain_sc,family=binomial)
lm_prob=predict(lin_mod,toptest_sc[2:4],type='response')
nn_pred=as.factor(ifelse(fznn>0.5,'Yes','No'))
lm_pred=as.factor(ifelse(lm_prob>0.5,'Yes','No'))
confusionMatrix(toptest_sc$default,nn_pred)
confusionMatrix(toptest_sc$default,lm_pred)
data_b<-read.csv("BeautyData.csv",header = TRUE)
plot(data_b$CourseEvals,data_b$BeautyScore,xlab='Course ratings',ylab='Beauty Score')
ll=lm(CourseEvals~BeautyScore,data=data_b)
summary(ll)
y=predict(ll,data_b[2])
par(mfrow=c(1,2))
plot(data_b$CourseEvals,y,xlab='Actual Course Ratings',ylab='Predicted Course Ratings',main='Actual vs Predicted Course Ratings')
abline(0,1,col='red')
plot(data_b$BeautyScore,data_b$CourseEvals-y,main='Residual Plot',xlab='Beauty Score',ylab='Residuals')
ll=lm(CourseEvals~.,data=data_b)
summary(ll)
y=predict(ll,data_b[2:6])
plot(data_b$CourseEvals,y,xlab='Actual Course Ratings',ylab='Predicted Course Ratings')
abline(0,1,col='red')
library(randomForest)
rf.beauty<-randomForest(CourseEvals~.,data = data_b,mtry=2,importance=TRUE)
yhat.rf<-predict(rf.beauty,newdata=data_b)
#plot(yhat.rf,Sales.test)
#abline(0,1)
RMSE.rf=sqrt(mean((yhat.rf-data_b$CourseEvals)^2))
importance(rf.beauty)
varImpPlot(rf.beauty)
data_h<-read.csv("MidCity.csv",header = TRUE)
data_h<-data_h[,2:8]
data_h['Nbhd1']<-as.integer(ifelse(data_h['Nbhd']==1,1,0))
#data_h['Nbhd2']<-as.integer(ifelse(data_h['Nbhd']==2,1,0))
data_h['Nbhd3']<-as.integer(ifelse(data_h['Nbhd']==3,1,0))
data_h['Brick']<-as.integer(ifelse(data_h['Brick']=='Yes',1,0))
data_h<-data_h[,2:9]
lin_mod<-lm(Price~.,data=data_h)
summary(lin_mod)
data_test <- data.frame(Offers = c(2,2,2,2),
SqFt = c(1790,1790,1790,1790),
Brick = c(0,1,1,1),
Bedrooms = c(2,2,2,2),
Bathrooms = c(2,2,2,2),
Nbhd1 = c(0,0,1,0),
Nbhd3 = c(1,1,0,0))
yhat<-predict(lin_mod,data_test)
test_cases <- data_test
test_cases[,'Predicted Price'] <- yhat
print(test_cases)
winequality.white <- read.csv("~/Desktop/winequality-white.csv", sep=";")
View(winequality.white)
wine_w<-wine_w %>% distinct()
library(tidyverse)
wine_w<-wine_w %>% distinct()
wine_w<-winequality.white %>% distinct()
#wine_r$alcohol2<-0
#wine_r$alcohol3<-0
wine_w$quality<-ifelse(wine_w$quality>6,1,0)
summary(wine_w)
glm.fit = glm(quality~.,data=wine_w,family=binomial)
summary(glm.fit)
wine_r$alcohol2<-wine_w$alcohol^2
wine_w$alcohol2<-wine_w$alcohol^2
wine_w$alcohol3<-wine_w$alcohol^3
glm.fit = glm(quality~.,data=wine_w,family=binomial)
summary(glm.fit)
wine_r <- read.csv("~/Desktop/winequality-red.csv", sep=";")
View(wine_r)
wine_r<-wine_r %>% distinct()
wine_r$quality<-ifelse(wine_r$quality>6,1,0)
#wine_r$alcohol2<-0
#wine_r$alcohol3<-0
glm.fit = glm(quality~.,data=wine_r,family=binomial)
summary(glm.fit)
winequality.white <- read.csv("~/Desktop/winequality-white.csv", sep=";")
View(winequality.white)
winequality.red <- read.csv("~/Desktop/winequality-red.csv", sep=";")
View(winequality.red)
summary(winequality.white)
summary(winequality.red)
library(tidyverse)
library(ggplot2)
library(Rtsne)
library(flexclust)
library(foreach)
library(caret)
set.seed(1)
wine = read.csv("../STA380/data/wine.csv")
setwd("~/Documents/GitHub/Intro-to-ML-Exam")
library(tidyverse)
library(ggplot2)
library(Rtsne)
library(flexclust)
library(foreach)
library(caret)
set.seed(1)
wine = read.csv("../STA380/data/wine.csv")
wine_d=wine[!duplicated(wine[, 1:11]), ]
wine_chems=wine_d[,c(1:11)]
wine_chems = scale(wine_chems, center=TRUE, scale=TRUE)
# a look at the correlation matrix
cor(wine_chems)
# a quick heatmap visualization
ggcorrplot::ggcorrplot(cor(wine_chems))
# looks a mess -- reorder the variables by hierarchical clustering
ggcorrplot::ggcorrplot(cor(wine_chems), hc.order = TRUE)
# Perform t-SNE
tsne_result <- Rtsne(wine_chems, perplexity = 100, dims = 2, verbose = TRUE)
# Plot the t-SNE result
plot(tsne_result$Y, col = "blue", pch = 20, main = "t-SNE Result")
# create a tidy summary of the loadings
tSNEwine = tsne_result$Y %>%
as.data.frame()
#Let's plot the wines with their colors
ggplot(tSNEwine, aes(x = V1, y = V2, color = wine_d$color)) +
geom_point(size = 3)
wine_c_data=tSNEwine[,c('V1','V2')]
# Run k-means with 2 clusters
clust1 = kmeans(wine_c_data, 2,nstart=25)
clust1$totss
clust1$betweenss
clust1$withinss
clust1$size
which.max(clust1$size)
#Since we have more white wines in the data set, let's name bigger cluster as white
wine_c_data$cc = ifelse(clust1$cluster==which.max(clust1$size),"white","red")
summary(clust1$cluster)
# Create the scatter plot
ggplot(wine_c_data, aes(x = V1, y = V2, color = cc)) +
geom_point(size = 3)  +
labs(title = "Wines with their predicted Color Categories", x = "V1", y = "V2")
confusionMatrix(as.factor(wine_c_data$cc),as.factor(wine_d$color))
library(tidyverse)
library(ggplot2)
library(caret)
library(flexclust)
library(foreach)
wine = read.csv("../STA380/data/wine.csv")
wine_chems=wine[,c(1:11)]
wine_chems = scale(wine_chems, center=TRUE, scale=TRUE)
# a look at the correlation matrix
cor(wine_chems)
# a quick heatmap visualizations
ggcorrplot::ggcorrplot(cor(wine_chems))
# looks a mess -- reorder the variables by hierarchical clustering
ggcorrplot::ggcorrplot(cor(wine_chems), hc.order = TRUE)
# Now look at PCA of the (average) survey responses.
# This is a common way to treat survey data
PCAwine = prcomp(wine_chems, scale=FALSE,rank. = 2)
## variance plot
plot(PCAwine)
summary(PCAwine)
# first few pcs
# try interpreting the loadings
# the question to ask is: "which variables does this load heavily on (positive and negatively)?"
round(PCAwine$rotation[,1:2],2)
# create a tidy summary of the loadings
loadings_summary = PCAwine$rotation %>%
as.data.frame() %>%
rownames_to_column('Question')
# This seems to pick out characteristics of
# sulfury vs vinegary wines?
loadings_summary %>%
select(Question, PC1) %>%
arrange(desc(PC1))
# this just seems to load positively on alcohol
# which is negatively correlated with density
loadings_summary %>%
select(Question, PC2) %>%
arrange(desc(PC2))
wine = merge(wine, PCAwine$x[,1:2], by="row.names")
wine = rename(wine, Show = Row.names)
#Let's plot the wines with their colors
ggplot(wine, aes(x = PC1, y = PC2, color = color)) +
geom_point(size = 3)
wine_c_data=wine[,c('PC1','PC2')]
# Run k-means with 2 clusters
clust1 = kmeans(wine_c_data, 2, nstart=25)
clust1$totss
clust1$betweenss
clust1$withinss
clust1$size
#Since we have more white wines in the data set, let's name bigger cluster as white
wine_c_data$cc = ifelse(clust1$cluster==which.max(clust1$size),"white","red")
summary(clust1$cluster)
# Create the scatter plot
ggplot(wine_c_data, aes(x = PC1, y = PC2, color = cc)) +
geom_point(size = 3)  +
labs(title = "Wines with their predicted Color Categories", x = "PC1", y = "PC2")
confusionMatrix(as.factor(wine_c_data$cc),as.factor(wine$color))
wine
wine$quality_c=ifelse(wine$quality>6,1,0)
wine$quality_c = ifelse(wine$quality>6,1,0)
#Let's plot the wines with their colors
ggplot(wine, aes(x = PC1, y = PC2, color = quality_c)) +
geom_point(size = 3)
wine$quality_c = as.factor(ifelse(wine$quality>6,1,0))
#Let's plot the wines with their colors
ggplot(wine, aes(x = PC1, y = PC2, color = quality_c)) +
geom_point(size = 3) +
labs(title = "Wines with their Actual Color Categories", x = "PC1", y = "PC2")
ggplot(wine, aes(x = PC1, y = PC2, color = quality_c)) +
geom_point(size = 3) +
labs(title = "Wines with their Actual Quality Ratings(1 means 7 or higher", x = "PC1", y = "PC2")
ggplot(wine, aes(x = PC1, y = PC2, color = quality_c)) +
geom_point(size = 3) +
labs(title = "Wines with their Actual Quality Ratings(1 means 7 or higher)", x = "PC1", y = "PC2")
tSNEwine$quality_c = as.factor(ifelse(wine_d$quality>6,1,0))
ggplot(tSNEwine, aes(x = V1, y = V2, color = quality_c)) +
geom_point(size = 3) +
labs(title = "Wines with their Actual Quality Ratings(1 means 7 or higher)", x = "PC1", y = "PC2")
